{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susichao/ml-project/blob/main/Copy_of_dga_ml_detec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je2ANg-E3dDT",
        "outputId": "94e30bb8-adb1-4d3c-eb1b-516120f782d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pushbullet.py\n",
            "  Downloading pushbullet.py-0.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pushbullet.py) (2.31.0)\n",
            "Collecting python-magic (from pushbullet.py)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: websocket-client>=0.53.0 in /usr/local/lib/python3.10/dist-packages (from pushbullet.py) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->pushbullet.py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->pushbullet.py) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->pushbullet.py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->pushbullet.py) (2024.2.2)\n",
            "Installing collected packages: python-magic, pushbullet.py\n",
            "Successfully installed pushbullet.py-0.12.0 python-magic-0.4.27\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.7)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.0.0 tldextract-5.1.2\n",
            "Collecting scapy\n",
            "  Downloading scapy-2.5.0.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: scapy\n",
            "  Building wheel for scapy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scapy: filename=scapy-2.5.0-py2.py3-none-any.whl size=1444327 sha256=206b1cf3cf8bea3b287f835faf57bf3d791d70817800e1f2ba318ee06577cd3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/b7/03/8344d8cf6695624746311bc0d389e9d05535ca83c35f90241d\n",
            "Successfully built scapy\n",
            "Installing collected packages: scapy\n",
            "Successfully installed scapy-2.5.0\n",
            "Collecting configparser\n",
            "  Downloading configparser-7.0.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: configparser\n",
            "Successfully installed configparser-7.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pushbullet.py\n",
        "!pip install tldextract\n",
        "!pip install scapy\n",
        "!pip install configparser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg0d3e2IHTFe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex7xgOpr8Prx",
        "outputId": "50074a0b-20c6-4717-cba1-19d2079c1a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (5.1.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.7)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.0.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tldextract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "qj9IZe7wHUMA",
        "outputId": "a87c3f48-9ea0-4e9e-bcea-13c345909924"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-10d03c8e9de7>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-10d03c8e9de7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m            \u001b[0;31m# Assuming the column name for label is 'malicious'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-10d03c8e9de7>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(domain)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_lexical_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_clustering_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-10d03c8e9de7>\u001b[0m in \u001b[0;36mextract_clustering_features\u001b[0;34m(domain)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1295\u001b[0m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tldextract\n",
        "import scipy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def load_data() -> pd.DataFrame:\n",
        "    if os.path.isfile('/content/sample_data/dga_data.csv'):\n",
        "        data = pd.read_csv('/content/sample_data/dga_data.csv')\n",
        "        return data\n",
        "    else:\n",
        "        raise ValueError(\"Error loading data. Please check the files.\")\n",
        "\n",
        "# Extract bigram features from domain names\n",
        "def extract_bigram_features(domain: str) -> list:\n",
        "    features = []\n",
        "    for i in range(len(domain) - 1):\n",
        "        features.append(domain[i:i+2])\n",
        "    return features\n",
        "\n",
        "# Calculate entropy of a domain name\n",
        "def calculate_entropy(fqdn_counts: dict) -> float:\n",
        "    entropy = 0\n",
        "    total_count = sum(fqdn_counts.values())\n",
        "    for count in fqdn_counts.values():\n",
        "        p = count / total_count\n",
        "        entropy += -p * np.log2(p)\n",
        "    return entropy\n",
        "\n",
        "# Extract lexical features from domain names\n",
        "def extract_lexical_features(domain: str) -> list:\n",
        "    features = []\n",
        "    if domain.count('.') == 1:\n",
        "        features.append(1)\n",
        "    else:\n",
        "        features.append(0)\n",
        "    if domain.islower():\n",
        "        features.append(1)\n",
        "    else:\n",
        "        features.append(0)\n",
        "    if domain.isalnum():\n",
        "        features.append(1)\n",
        "    else:\n",
        "        features.append(0)\n",
        "    if domain.count('-') > 0:\n",
        "        features.append(1)\n",
        "    else:\n",
        "        features.append(0)\n",
        "    if domain.count('_') > 0:\n",
        "        features.append(1)\n",
        "    else:\n",
        "        features.append(0)\n",
        "    if domain.count('0') == len(domain):\n",
        "        features.append(1)\n",
        "    else:\n",
        "        features.append(0)\n",
        "    return features\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def preprocess_domain(domain: str) -> str:\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    domain = ' '.join([word for word in domain.split() if word not in stop_words])\n",
        "\n",
        "    # Remove non-alphabetical characters\n",
        "    domain = re.sub(r'[^a-zA-Z]', '', domain)\n",
        "\n",
        "    return domain\n",
        "\n",
        "def extract_clustering_features(domain: str) -> list:\n",
        "    if not domain:\n",
        "        return []\n",
        "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))\n",
        "    X = vectorizer.fit_transform([domain])\n",
        "    return X.toarray()[0]\n",
        "\n",
        "# Extract features from domain names\n",
        "\n",
        "def extract_features(domain: str) -> list:\n",
        "    extract_result = tldextract.extract(domain)\n",
        "    fqdn_counts = extract_result.fqdn_counts if hasattr(extract_result, 'fqdn_counts') else {}\n",
        "    entropy = calculate_entropy(fqdn_counts)\n",
        "\n",
        "    features = []\n",
        "    features.extend(extract_bigram_features(domain))\n",
        "    features.append(entropy)\n",
        "    features.extend(extract_lexical_features(domain))\n",
        "    features.extend(extract_clustering_features(domain))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    data = load_data()\n",
        "    if data is not None:\n",
        "        X, y = [], []\n",
        "        for index, row in data.iterrows():\n",
        "            domain = row['domain']\n",
        "            features = extract_features(domain)\n",
        "            X.append(features)\n",
        "           # Assuming the column name for label is 'malicious'\n",
        "            label = 1 if row['isDGA'] == 1 else 0\n",
        "            y.append(label)\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train the ML model\n",
        "        model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"Model accuracy:\", accuracy)\n",
        "    else:\n",
        "        raise ValueError(\"Error loading data. Model training failed.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3sB9pVDKVwx",
        "outputId": "012b6981-5d0c-49e9-d0b6-431c66f3fe71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['url', 'type'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/sample_data/malicious_phish.csv')\n",
        "print(data.columns)\n"
      ]
    },
    {
      "source": [
        "def has_valid_ngrams(domain, n=2):\n",
        "    # Convert the domain to a string\n",
        "    domain = str(domain)\n",
        "\n",
        "    # Extract n-grams from the domain\n",
        "    ngrams_list = list(ngrams(domain, n))\n",
        "\n",
        "    # Check if the list of n-grams is empty\n",
        "    if not ngrams_list:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XX3UzeIwM5Bu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/sample_data/malicious_phish.csv')\n",
        "\n",
        "# Check if each domain has valid 2-grams\n",
        "df['has_valid_2grams'] = df['url'].apply(has_valid_ngrams)\n",
        "\n",
        "# Print the results\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B7YcwNyM9k-",
        "outputId": "18b956dc-e99a-4dd8-f542-dffe9a7cccec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 url        type  \\\n",
            "0                                   br-icloud.com.br    phishing   \n",
            "1                mp3raid.com/music/krizz_kaliko.html      benign   \n",
            "2                    bopsecrets.org/rexroth/cr/1.htm      benign   \n",
            "3  http://www.garage-pirenne.be/index.php?option=...  defacement   \n",
            "4  http://adventure-nicaragua.net/index.php?optio...  defacement   \n",
            "\n",
            "   has_valid_2grams  \n",
            "0              True  \n",
            "1              True  \n",
            "2              True  \n",
            "3              True  \n",
            "4              True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tldextract\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import logging"
      ],
      "metadata": {
        "id": "C5IPp_C7T7AB"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s -%(lineno)s')"
      ],
      "metadata": {
        "id": "60nlTW9-UJ36"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the data from the specified file path.\"\"\"\n",
        "    file_path = '/content/sample_data/malicious_phish.csv'\n",
        "    if os.path.isfile(file_path):\n",
        "        data = pd.read_csv(file_path)\n",
        "        return data\n",
        "    else:\n",
        "        raise ValueError(f\"Error loading data. File not found: {file_path}\")"
      ],
      "metadata": {
        "id": "vwFBkNncUNTv"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(fqdn_counts: dict) -> float:\n",
        "    \"\"\"Calculate the entropy of a domain name based on the frequency counts of its components.\"\"\"\n",
        "    entropy = 0\n",
        "    total_count = sum(fqdn_counts.values())\n",
        "    for count in fqdn_counts.values():\n",
        "        p = count / total_count\n",
        "        entropy += -p * np.log2(p)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "_xjUMGsSUS22"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(domain: str) -> list:\n",
        "    \"\"\"Extract features from a domain name.\"\"\"\n",
        "    try:\n",
        "        extract_result = tldextract.extract(domain)\n",
        "        fqdn_counts = extract_result.fqdn_counts if hasattr(extract_result, 'fqdn_counts') else {}\n",
        "        entropy = calculate_entropy(fqdn_counts)\n",
        "\n",
        "        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
        "        X = vectorizer.fit_transform([domain])\n",
        "        features = list(X.toarray()[0])\n",
        "        features.append(entropy)\n",
        "\n",
        "        return features\n",
        "    except ValueError as e:\n",
        "        if \"empty vocabulary\" in str(e):\n",
        "            logging.warning(f\"Empty vocabulary for domain '{domain}'. Using entropy as the only feature.\")\n",
        "            extract_result = tldextract.extract(domain)\n",
        "            fqdn_counts = extract_result.fqdn_counts if hasattr(extract_result, 'fqdn_counts') else {}\n",
        "            entropy = calculate_entropy(fqdn_counts)\n",
        "            return [entropy]\n",
        "        else:\n",
        "            raise e"
      ],
      "metadata": {
        "id": "h8cqfJPxUVqm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(data: pd.DataFrame) -> None:\n",
        "    \"\"\"Train and evaluate the machine learning model.\"\"\"\n",
        "    X, y = [], []\n",
        "    for index, row in data.iterrows():\n",
        "        domain = row['url']\n",
        "        features = extract_features(domain)\n",
        "        X.append(features)\n",
        "        label = 1 if row['type'] == 1 else 0\n",
        "        y.append(label)\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    models = [\n",
        "        RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
        "        LogisticRegression(random_state=42)\n",
        "    ]\n",
        "\n",
        "    for model in models:\n",
        "        logging.info(f\"Training {model.__class__.__name__}...\")\n",
        "        # Cross-validation\n",
        "        scores = cross_val_score(model, X, y, cv=5)\n",
        "        logging.info(f\"Cross-validation scores: {scores}\")\n",
        "        logging.info(f\"Mean accuracy: {scores.mean():.3f}\")\n",
        "         # Fit the model\n",
        "        model.fit(X_train, y_train)\n",
        "        # Evaluate the model\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        logging.info(f\"Test accuracy: {accuracy:.3f}\")\n",
        "        logging.info(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0JpGkEypUcGG"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        data = load_data()\n",
        "        train_model(data)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "8zHoiL8jUyLY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP3vNTEKVkbobaq8VsbP3GV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}