{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6LFtn91lEa4RqXMk/0C3/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susichao/ml-project/blob/main/DGA_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWSM9YVwpMpP"
      },
      "outputs": [],
      "source": [
        "!pip install pushbullet.py\n",
        "!pip install tldextract\n",
        "!pip install scapy\n",
        "!pip install configparser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries and modules"
      ],
      "metadata": {
        "id": "Is_0E6SluSOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import tldextract\n",
        "import scipy\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "LvzIh0DquWWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a Settings.conf file"
      ],
      "metadata": {
        "id": "4yK9qnaX_j_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config['Settings'] = {'baseline': '0.5', 'total_bigrams': '1000'}\n",
        "\n",
        "with open('settings.conf', 'w') as configfile:\n",
        "    config.write(configfile)\n"
      ],
      "metadata": {
        "id": "zL6eLHut_bd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load training data and settings"
      ],
      "metadata": {
        "id": "XYUE5ZLCu-0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_data_and_settings() -> tuple:\n",
        "    if os.path.isfile('data/settings.conf'):\n",
        "        data = pd.read_csv('your_data.csv')  # Replace 'your_data.csv' with your CSV file path\n",
        "        baseline, total_bigrams_settings = load_settings()  # Assuming load_settings() loads settings from 'settings.conf'\n",
        "        return data, baseline, total_bigrams_settings\n",
        "    else:\n",
        "        raise ValueError(\"Error loading data. Please check the files.\")"
      ],
      "metadata": {
        "id": "9uSrRrC9vCYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import configparser\n",
        "\n",
        "# def load_settings():\n",
        "#     config = configparser.ConfigParser()\n",
        "#     config.read_string(uploaded['settings.conf'].decode('utf-8'))\n",
        "\n",
        "#     baseline = float(config['Settings']['baseline'])\n",
        "#     total_bigrams_settings = int(config['Settings']['total_bigrams'])\n",
        "\n",
        "#     return baseline, total_bigrams_settings\n",
        "\n",
        "# baseline, total_bigrams_settings = load_settings()\n",
        "# print(\"Baseline:\", baseline)\n",
        "# print(\"Total Bigrams Settings:\", total_bigrams_settings)\n"
      ],
      "metadata": {
        "id": "TDJJDRtF_h8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "entropy of a domain name"
      ],
      "metadata": {
        "id": "EVWA8XJmvIdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "# from collections import Counter\n",
        "\n",
        "# def calculate_entropy(domain: str) -> float:\n",
        "#     char_freq = Counter(domain)\n",
        "\n",
        "#     domain_length = len(domain)\n",
        "#     prob_dist = [char_freq[char] / domain_length for char in set(domain)]\n",
        "\n",
        "#     entropy_val = -sum(p * math.log2(p) for p in prob_dist)\n",
        "\n",
        "#     return entropy_val\n",
        "\n",
        "# def calculate_entropy_for_dataframe(data: pd.DataFrame, domain_column: str) -> pd.Series:\n",
        "#     entropies = data[domain_column].apply(calculate_entropy)\n",
        "#     return entropies\n",
        "\n",
        "# data = pd.read_csv('your_data.csv')  # Load your CSV dataset\n",
        "# entropies = calculate_entropy_for_dataframe(data, 'domain')\n",
        "\n",
        "# # Add entropy values to the DataFrame\n",
        "# data['entropy'] = entropies\n",
        "\n",
        "# # Print the DataFrame with entropy values\n",
        "# print(data)"
      ],
      "metadata": {
        "id": "bsJeK1KtvO_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lexical features from domain names"
      ],
      "metadata": {
        "id": "IPzbfE7HvYy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Implement lexical feature extraction logic\n",
        "\n",
        "def extract_lexical_features(domain_list):\n",
        "    features = []\n",
        "\n",
        "    for domain in domain_list:\n",
        "        # Length of the domain\n",
        "        length = len(domain)\n",
        "\n",
        "        # Count of numbers, letters, special characters\n",
        "        num_count = sum(char.isdigit() for char in domain)\n",
        "        letter_count = sum(char.isalpha() for char in domain)\n",
        "        special_char_count = length - num_count - letter_count\n",
        "\n",
        "        # Number of vowels and consonants\n",
        "        vowels = sum(char in 'aeiou' for char in domain.lower())\n",
        "        consonants = letter_count - vowels\n",
        "\n",
        "        # Count of consecutive characters\n",
        "        consecutive_count = max(sum(1 for _ in g) for _, g in groupby(domain.lower()))\n",
        "\n",
        "        # Ratio of vowels to consonants\n",
        "        if consonants != 0:\n",
        "            vowel_consonant_ratio = vowels / consonants\n",
        "        else:\n",
        "            vowel_consonant_ratio = np.nan\n",
        "\n",
        "        # Ratio of numbers to letters\n",
        "        if letter_count != 0:\n",
        "            number_letter_ratio = num_count / letter_count\n",
        "        else:\n",
        "            number_letter_ratio = np.nan\n",
        "\n",
        "        # Add features to the feature vector\n",
        "        feature_vector = [length, num_count, letter_count, special_char_count,\n",
        "                          vowels, consonants, consecutive_count, vowel_consonant_ratio,\n",
        "                          number_letter_ratio]\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "GcgNQyH_31ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clustering features from domain names"
      ],
      "metadata": {
        "id": "WAUHsQob37F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_clustering_features(domain: str) -> list:\n",
        "    # Implement clustering feature extraction logic here\n",
        "    # For example, use K-Means clustering\n",
        "    kmeans = KMeans(n_clusters=5)\n",
        "    features = kmeans.fit_transform([domain])\n",
        "    return features"
      ],
      "metadata": {
        "id": "5one14u34CT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " extract bigram features from domain names"
      ],
      "metadata": {
        "id": "KOGv91094TQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_bigram_features(domain: str) -> list:\n",
        "    features = []\n",
        "    for i in range(len(domain) - 1):\n",
        "        features.append(domain[i:i+2])\n",
        "    return features"
      ],
      "metadata": {
        "id": "VIB2uj4y4Zmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting features from domain name\n"
      ],
      "metadata": {
        "id": "fs-4JTT95c7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(domain: str) -> list:\n",
        "    features = []\n",
        "    features.extend(extract_bigram_features(domain))\n",
        "    features.append(calculate_entropy(domain))\n",
        "    features.extend(extract_lexical_features(domain))\n",
        "    features.extend(extract_clustering_features(domain))\n",
        "    return features"
      ],
      "metadata": {
        "id": "9bIjO-oQ5iHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the ML model"
      ],
      "metadata": {
        "id": "TX-g_JdL5ksh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    bigram_dict, baseline, total_bigrams_settings = load_data_and_settings()\n",
        "    if bigram_dict:\n",
        "        X, y = [], []\n",
        "        data = pd.read_csv('data/training_data.txt')\n",
        "        for domain in data:\n",
        "            features = extract_features(domain)\n",
        "            X.append(features)\n",
        "            label = 1 if check_domain(domain) == 1 else 0\n",
        "            y.append(label)"
      ],
      "metadata": {
        "id": "7a9sZQMo5qI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectorize features"
      ],
      "metadata": {
        "id": "v-_dvK3l5uwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
        "        X = vectorizer.fit_transform(X)"
      ],
      "metadata": {
        "id": "v77ICWtW5x73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split data into training and test set"
      ],
      "metadata": {
        "id": "-Sfro9Ha501E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "oelhcbnR55Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Evaluation"
      ],
      "metadata": {
        "id": "8KaDeAZb5_Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Train the ML model\n",
        "        model = RandomForestClassifier()\n",
        "        model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "QCpgDyLg6D-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Evaluate the model\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(\"Model accuracy:\", accuracy)\n",
        "    else:\n",
        "        raise ValueError(\"Error loading data. Model training failed.\")\n"
      ],
      "metadata": {
        "id": "FOvlGDWT6IXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to execute the ML model training\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "id": "wzy3a_Cb6LSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbVRzUN36L-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}